{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import inspect\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, parameters): \n",
    "    \"\"\"\n",
    "    Runs forward propagation on z and parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    z -- noise\n",
    "    parameters -- neural net weights\n",
    "    \n",
    "    Returns:\n",
    "    z -- output of last activation layer\n",
    "    \"\"\"\n",
    "    z = z.t()\n",
    "    z = parameters['W1'] @ z\n",
    "    z = F.relu(z)\n",
    "    z = parameters['W2'] @ z\n",
    "    z = F.relu(z)\n",
    "    return z.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.sort(torch.Tensor(16384, 1).uniform_(-1, 1), dim=0)[0]\n",
    "Y = torch.sort(torch.Tensor(16384, 1).normal_(), dim=0)[0]\n",
    "dataset = torch.utils.data.TensorDataset(X, Y, )\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "parameters = {'W1': torch.nn.init.xavier_uniform_(torch.zeros((64,1), requires_grad=True)),\n",
    "              'b1': torch.nn.init.xavier_uniform_(torch.zeros((64, 1), requires_grad=True)),\n",
    "    'W2': torch.nn.init.xavier_uniform_(torch.zeros((1, 64),requires_grad=True))\n",
    "              'b2': torch.nn.init.xavier_uniform_(torch.zeros((64, 1), requires_grad=True)),\n",
    "}\n",
    "criterion = torch.nn.MSELoss(reduction='elementwise_mean')\n",
    "optimizer = optim.SGD([theta_g], lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 100, Loss: 1.0857272088527679\n",
      "Epoch: 0, Batch: 200, Loss: 1.1056722819805145\n",
      "Epoch: 0, Batch: 300, Loss: 1.0655073934793473\n",
      "Epoch: 0, Batch: 400, Loss: 1.0729828321933745\n",
      "Epoch: 0, Batch: 500, Loss: 1.0787764227390289\n",
      "Epoch: 1, Batch: 100, Loss: 1.1177278262376786\n",
      "Epoch: 1, Batch: 200, Loss: 1.0437852710485458\n",
      "Epoch: 1, Batch: 300, Loss: 1.052260610461235\n",
      "Epoch: 1, Batch: 400, Loss: 1.079850641489029\n",
      "Epoch: 1, Batch: 500, Loss: 1.1251916426420212\n",
      "Epoch: 2, Batch: 100, Loss: 1.1052210342884063\n",
      "Epoch: 2, Batch: 200, Loss: 1.0411426174640654\n",
      "Epoch: 2, Batch: 300, Loss: 1.1065278035402297\n",
      "Epoch: 2, Batch: 400, Loss: 1.0709260088205337\n",
      "Epoch: 2, Batch: 500, Loss: 1.0806153458356857\n",
      "Epoch: 3, Batch: 100, Loss: 1.0712980872392655\n",
      "Epoch: 3, Batch: 200, Loss: 1.081751475930214\n",
      "Epoch: 3, Batch: 300, Loss: 1.0749504637718201\n",
      "Epoch: 3, Batch: 400, Loss: 1.0863949391245842\n",
      "Epoch: 3, Batch: 500, Loss: 1.102592784166336\n",
      "Epoch: 4, Batch: 100, Loss: 1.1454333102703094\n",
      "Epoch: 4, Batch: 200, Loss: 1.0838271617889403\n",
      "Epoch: 4, Batch: 300, Loss: 1.0933879405260085\n",
      "Epoch: 4, Batch: 400, Loss: 1.0421867167949677\n",
      "Epoch: 4, Batch: 500, Loss: 1.0623278141021728\n",
      "Epoch: 5, Batch: 100, Loss: 1.0917274874448777\n",
      "Epoch: 5, Batch: 200, Loss: 1.0733312255144118\n",
      "Epoch: 5, Batch: 300, Loss: 1.1097129899263383\n",
      "Epoch: 5, Batch: 400, Loss: 1.083344955444336\n",
      "Epoch: 5, Batch: 500, Loss: 1.058207185268402\n",
      "Epoch: 6, Batch: 100, Loss: 1.0662948834896087\n",
      "Epoch: 6, Batch: 200, Loss: 1.0909517273306846\n",
      "Epoch: 6, Batch: 300, Loss: 1.0665137773752214\n",
      "Epoch: 6, Batch: 400, Loss: 1.0989396059513092\n",
      "Epoch: 6, Batch: 500, Loss: 1.0895441377162933\n",
      "Epoch: 7, Batch: 100, Loss: 1.0798853841423988\n",
      "Epoch: 7, Batch: 200, Loss: 1.1082001155614853\n",
      "Epoch: 7, Batch: 300, Loss: 1.1092058855295182\n",
      "Epoch: 7, Batch: 400, Loss: 1.0578788262605667\n",
      "Epoch: 7, Batch: 500, Loss: 1.0462669736146928\n",
      "Epoch: 8, Batch: 100, Loss: 1.1279440289735794\n",
      "Epoch: 8, Batch: 200, Loss: 1.078686661720276\n",
      "Epoch: 8, Batch: 300, Loss: 1.0835520020127296\n",
      "Epoch: 8, Batch: 400, Loss: 1.0430893301963806\n",
      "Epoch: 8, Batch: 500, Loss: 1.072498201727867\n",
      "Epoch: 9, Batch: 100, Loss: 1.0720432934165\n",
      "Epoch: 9, Batch: 200, Loss: 1.069811835885048\n",
      "Epoch: 9, Batch: 300, Loss: 1.0832263451814652\n",
      "Epoch: 9, Batch: 400, Loss: 1.1112681674957274\n",
      "Epoch: 9, Batch: 500, Loss: 1.0791389885544778\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad() # Why...\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        outputs = generator(inputs, parameters)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if (batch_idx % 100 == 0 and batch_idx > 0):\n",
    "            print(F\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {running_loss/100}\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(z.numpy())\n",
    "plot(y.numpy())\n",
    "plot(generator(X, parameters).data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-75-81eba2ed299d>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-75-81eba2ed299d>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    #Returns scalar probability that x comes from data rather than generator\u001b[0m\n\u001b[0m                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def discriminator(x, theta_d): \n",
    "    #x:=input sample\n",
    "    #theta_d:=neural net params\n",
    "    #Returns scalar probability that x comes from data rather than generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
